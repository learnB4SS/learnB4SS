---
title: "Application to regression IV - Leveling up to hierarchical models"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Application to regression IV - Leveling up to hierarchical models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = TRUE,
  eval = FALSE
)
```

```{r setup}

# libraries
library(learnB4SS)
library(brms)
library(tidyverse)
library(tidybayes)
library(bayestestR)

# data
polite <- learnB4SS::polite
```

# Walkthrough

## Introduction

Until now, we have only dealt with very simple models, most of which are just not particularly relevant for what we actually do in our research, right? So in this section, we will level up. We will add additional parameters to our regression in the form of fixed effects and random effect. Sounds much more like the stuff you want to do, right? We are going to estimate these parameters and specify appropriate priors for them. After this session you will be much closer to being an operational Bayesian for your actual research.

## A simple model

Okay let's think about this. Here is the model and its priors that we have so far looked at:

```{r simpleModel, message = F}

# specify priors
priors_simple <- c(
  prior(normal(0, 15), class = Intercept),
  prior(normal(0, 10), class = b, coef = attitudepol),
  prior(cauchy(0, 1), class = sigma)
)


# CHANGE ACCORDINGLY
simple_model <- brm(
  articulation_rate ~ 
    attitude,  
    data = polite,
    prior = priors_simple
  )

```

Let us briefly remind ourselves, what the results look like. 

```{r simpleModel_post, message = F}

# Extract posterior coefficient and plot
post_simple_plot <- posterior_samples(simple_model) %>% 
  # select the columns with the relevant regression coefficients
  select(b_attitudepol) %>% 
  ggplot(., aes(x = b_attitudepol), fill = b4ss_colors[[1]]) +
    stat_halfeye() +
    geom_vline(xintercept = 0, lty = "dashed") +
  labs(x = "articulation rate")

post_simple_plot

```

If you want to plot something more traditional, i.e. the estimated values for polite and informal speech productions, we need to indulge in some more data wrangling. The information is all there, but needs to be extracted with a hand full of tidyverse functions. Let's wrangle the posteriors such that we can plot the posterior distribution for polite and informal productions: 

```{r traditional_plotting, message = F}

# Extract posterior coefficient
post_simple2 <- posterior_samples(simple_model) %>% 
  # select the columns with the relevant regression coefficients
  select(b_Intercept, b_attitudepol) %>% 
  # calculate the polite group (intercept plus polite coefficient)
  mutate(polite = b_Intercept + b_attitudepol) %>% 
  # rename the intercept
  rename(informal = b_Intercept) %>% 
  # get rid of the remainder
  select(-b_attitudepol) %>%
  # make tidy long format
  pivot_longer(cols = c(polite, informal), 
               names_to = "coef", values_to = "posterior" )

# Plot posteriors
post_simple_plot2 <- post_simple2 %>%
  ggplot(., aes(x = posterior, group = coef,fill = coef)) + 
           stat_halfeye(alpha = 0.5) +
  labs(x = "articulation rate")

post_simple_plot2

```

## Adding predictors 

This model estimates the effect of `attitude` on articulation rate (`articulation_rate`). That-is-it. But what if we also want to estimate the differences between attitude across different experimental tasks? Let's add `task` as a predictor and add a prior for it as well. Let's stick to weakly informative priors again, centered on zero (we expect no difference of articulation rate between different tasks and acknowledge the possibility of some variance around that value).

```{r multiple_model, message = F}

# get prior
get_prior(
 formula = 
   articulation_rate ~ attitude + task,
  data = polite
)

# specify priors
priors_multiple <- c(
  prior(normal(0, 15), class = Intercept),
  prior(normal(0, 10), class = b, coef = attitudepol),
  prior(normal(0, 10), class = b, coef = tasknot),
  prior(normal(0, 10), class = sigma)
)


# Run model
multiple_model <- brm(
  articulation_rate ~ 
    attitude + task,
  data = polite,
  prior = priors_multiple)

summary(multiple_model)

```

Looking at the summary of the model, we get a table with two coefficients for the population parameters `attitudepol` and `tasknot`. Let's remind ourselves that these reflect the change in articulation rate from the reference level (attitude = informal (`inf`), task = dialog completion task (`dct`)) to attitude = polite (`attitudepol`) and to task = note (`tasknot`), respectively.

We can inspect posteriors for both predictors just like we did before:

```{r multiple_model_post, message = F}

# Extract posterior coefficient attitudepol
post_multiple_attitude <- multiple_model %>%
  spread_draws(b_attitudepol, b_tasknot) %>% 
  ggplot(.) + 
    stat_halfeye(aes(x = b_attitudepol)) +
    labs(x = "articulation rate")
 
# Extract posterior coefficient tasknot 
post_multiple_task <- multiple_model %>%
  spread_draws(b_attitudepol, b_tasknot) %>% 
  ggplot(.) + 
    stat_halfeye(aes(x = b_tasknot)) +
    labs(x = "articulation rate")

# plot
library(patchwork)
post_multiple_attitude + post_multiple_task

```

## Adding random effects

So running Bayesian regression models is really not much different from running frequentist regression models. Practically, the only difference so far is that we specify prior knowledge. Conceptually, we obviously interpret the output differently in terms of the inference we draw, but practically this should be business as usual for you.

Now, we all know that this model is not taking into account the fact that data points are not independent from each other (for a quick refresher on this topics, see ####). There were multiple speakers that produced multiple data points, so observations from these speakers are not independent and we need to take that into account for robust inference. So let's also add an appropriate random effect structure, including a by-subject random intercept as well as a by-subject random slope for attitude.

We can write down the code to run this model easily, because we are all familiar with the `lme4` syntax. `brms` syntax handles random effect structure exactly like `lme4` like so:

```{r complexModel, f}

# CHANGE ACCORDINGLY
complex_model <- brm(
  articulation_rate ~ 
    attitude + task +
    # add random intercept
    (1 | subject) +
    # add random slope
    (0 + attitude | subject),  
   data = polite,
   prior = priors_complex,
   seed = 999
)

summary(complex_model)

```

Looks familiar? 

Let us walk through the summary here. We now have one part of the table called "Group-Level Effects". This part of the table gives us the models estimates of four parameters: How much subjects vary in general (`sd(Intercept)`), how much they differ in the informal condition (`sd(attitudeinf)`), how much subjects vary in the polite condition (`sd(attitudepol)`), and what the correlation between `sd(attitudeinf)` and `sd(attitudepol)` is. As before, for all of these parameters, we receive an `Estimate` which is the mean of the posterior distribution and a range of plausible values within the 95% Credible Interval (`l-95% CI` - `u-95% CI`). Except for the correlation, the estimates are bound by 0, i.e. the variance can only be positive. The correlation coefficient can vary between -1 and 1.

The second part of the summary table, i.e. the Population-Level Effects, did not change and summarizes our regression coefficients for the fixed effects. Just like before.

But what about priors? Do we need to specify priors for these new elements as well? And if so how?

Generally, we encourage you to specify priors for all elements of your model, so let us try this as well. We need to specify the four parameters for the Group-Level Effects.

```{r complexModel_priors, f}

# get prior
get_prior(
 formula = 
   articulation_rate ~ 
    attitude + task +
    (1 | subject) +
    (0 + attitude | subject),  
   data = polite
)

# specify priors
priors_complex <- c(
  prior(normal(0, 15), class = Intercept),
  prior(normal(0, 10), class = b, coef = attitudepol),
  prior(normal(0, 10), class = b, coef = tasknot),
  # specify weakly informative prior for the random effects (slopes)
  prior(normal(0, 10), class = sd, coef = Intercept, group = subject),
  prior(normal(0, 10), class = sd, coef = attitudeinf, group = subject),
  prior(normal(0, 10), class = sd, coef = attitudepol, group = subject),
  # specify weakly informative prior for the correlation between random intercept and slope
  prior(lkj_corr_cholesky(2), class = L, group = subject),
  prior(normal(0, 10), class = sigma)
)


# Run model
complex_model <- brm(
  articulation_rate ~ 
    attitude + task +
    # add random intercept
    (1 | subject) +
    # add random slope
    (0 + attitude | subject),  
   data = polite,
   prior = priors_complex,
   seed = 999
)

summary(complex_model)

```

There we go. We have run a linear mixed effects model within the Bayesian framework. We specified priors for both random and fixed effects. What is left is interpreting the output.

```{r complexModel_post, message = F}

# Extract posterior coefficient politeness
post_complex <- complex_model %>%
  spread_draws(b_attitudepol, b_tasknot) 

# plot
ggplot(post_complex) + 
  stat_halfeye(aes(x = b_attitudepol)) +
  labs(x = "articulation rate")

```

## Making inference

Now let's use these posteriors to draw probability inferences. What is the 95% credible interval for the `attitudepol` coefficient and what is its probability of direction?

```{r p_direction, message = F}

# extract 95% HDI
hdi(post_complex$b_attitudepol)

# extract probability of direction
p_direction(post_complex$b_attitudepol)

```

Fantastic! So our best estimate of the effect of attitude has a 95% CrI between [-0.80, -0.01] and a 0.98 probability of being negative. Although the majority of plausible values are negative, positive values are still a posibility, given the data, the model, and the priors. 



# Exercise

Now it's your turn: Our goal is it to run the following model: `f0mn ~ attitude + gender + (1 | subject) + (0 + attitude | subject)`

(a) First, see which parameters we need to specify with priors using the `get_prior()` function. (If you are unsure, see our code examples above)

```{r get_prior_ex, message = F}

# get prior
get_prior(
 formula = 
   f0mn ~ 
   attitude + gender +
   (1 | subject) + 
   (0 + attitude | subject),
  data = polite
)

```

(b) Now specify weakly informative priors for all parameters. For the intercept, chose a normal prior with mean = 0 and sd = 300. For fixed effects, random effects and residual variance go with something like a normal prior with mean = 0 and sd = 100.

```{r get_prior_ex_1, message = F}

# specify priors
priors_ex <- c(
  prior(normal(0, 300), class = Intercept),
  prior(normal(0, 100), class = b, coef = attitudepol),
  prior(normal(0, 100), class = b, coef = genderM),
  # specify weakly informative prior for the random effects (intercepts)
  prior(normal(0, 100), class = sd, group = subject),
   # specify weakly informative prior for the random effects (slopes)
  prior(normal(0, 100), class = sd, coef = Intercept, group = subject),
  prior(normal(0, 100), class = sd, coef = attitudeinf, group = subject),
  prior(normal(0, 100), class = sd, coef = attitudepol, group = subject),
  # specify weakly informative prior for the correlation between random intercept and slope
  prior(lkj_corr_cholesky(2), class = L, group = subject),
  prior(normal(0, 100), class = sigma)
)

```

(c) Now run the model with your specified priors. Use `seed = 999` in order for all of you to get the same results.

```{r get_prior_ex_2, message = F}

# Run model
ex_model <- brm(
  f0mn ~ 
    attitude + gender +
    # add random intercept
    (1 | subject) +
    # add random slope
    (0 + attitude | subject),  
   data = polite,
   prior = priors_ex,
   seed = 999
)

summary(ex_model)

```

(d) extract the posteriors for the coefficient attitudepol and plot it.

```{r ex_model_post, message = F}

# Extract posterior coefficient politeness
ex_post <- ex_model %>%
  spread_draws(b_attitudepol)

# plot
ggplot(ex_post) + 
  stat_halfeye(aes(x = b_attitudepol)) +
  labs(x = "fundamental frequency in Hz")


```

(e) Now let's use the posteriors to draw probability inferences. What is the 95% credible interval for the `attitudepol` coefficient and what is its probability of direction?

```{r p_direction, message = F}

# extract 95% HDI
hdi(ex_post$b_attitudepol)

# extract probability of direction
p_direction(ex_post$b_attitudepol)

```

[{"path":"https://learnb4ss.github.io/learnB4SS/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2020 learnB4SS authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/contrasts.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Factors, coding and contrasts","text":"’s seems bit terminological mix-wild, first present terminological set used throughout vignette. Categorical variables R generally stored using factors. factor vector values categorical variable. possible values factor called levels R. observation factor, vector specifies level observation. example, let’s assume data set information dinosaurs one column specifies dinosaur’s diet: carnivore, herbivore, omnivore. R, column can coded factor: accustomed using factors regression models sometimes forget regressions work numbers work categorical variables. fit regression model categorical variables, first converted numbers. process conversion called coding. One type coding dummy variable coding simply dummy coding. consists assigning 0s 1s levels variable. Let’s go simple example dummy coding categorical variable 2 levels: metropolitan rural. simple way coding categorical variable number assign 0 one level 1 level. example: R, dummy coding done hood using factors, don’t worry conversion. categorical variable 3 levels instead 2, need work-around order code 3-level factor 0s 1s (can’t use higher numbers reasons see later). three levels, can code variable using two numeric variables (instead just one). Going back dinosaur’s diet example, can use: One variable codes whether dinosaur carnivore 0 herbivore 1. One variable codes whether dinosaur carnivore 0 omnivore 1. Let call first variable dummy_1 second variable dummy_2. : dummy_1 0 dummy_2 also 0, dinosaur carnivore. dummy_1 1 dummy_2 0, dinosaur herbivore. dummy_1 0 dummy_2 1, dinosaur omnivore. following factor (repeated ): can coded : doesn’t make much sense, try figure checking value two columns row following code: factor 4 levels? can code 3 dummy variables. 5 levels? Use 4 dummy variables. number dummy variables needed equal number levels minus 1 (\\(n_{dummy} = n_{levels} - 1\\)).","code":"factor(c(\"carnivore\", \"carnivore\", \"herbivore\", \"omnivore\", \"herbivore\")) #> [1] carnivore carnivore herbivore omnivore  herbivore #> Levels: carnivore herbivore omnivore factor(c(\"rural\", \"rural\", \"metropolitan\", \"rural\")) #> [1] rural        rural        metropolitan rural        #> Levels: metropolitan rural # dummy coded c(0, 0, 1, 0) #> [1] 0 0 1 0 factor(c(\"carnivore\", \"carnivore\", \"herbivore\", \"omnivore\", \"herbivore\")) #> [1] carnivore carnivore herbivore omnivore  herbivore #> Levels: carnivore herbivore omnivore dummy_1 <- c(0, 0, 1, 0, 1)     # carnivore (0) or herbivore (1)? dummy_2 <- c(0, 0, 0, 1, 0)     # carnivore (0) or omnivore (1)?  library(tidyverse)  tibble(   dummy_1, dummy_2 ) #> # A tibble: 5 × 2 #>   dummy_1 dummy_2 #>     <dbl>   <dbl> #> 1       0       0 #> 2       0       0 #> 3       1       0 #> 4       0       1 #> 5       1       0 # case_when() is a very helpful function from dplyr!  case_when(   dummy_1 == 0 & dummy_2 == 0 ~ \"carnivore\",   dummy_1 == 1 & dummy_2 == 0 ~ \"herbivore\",   dummy_1 == 0 & dummy_2 == 1 ~ \"omnivore\", )"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/contrasts.html","id":"summing-up","dir":"Articles","previous_headings":"Introduction","what":"Summing up","title":"Factors, coding and contrasts","text":"sum : Factors vectors code categorical variables. values factor called levels. Regression models work directly factors, coded using numbers. Dummy coding one way coding factors numbers using one numeric variables 0s 1s.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/contrasts.html","id":"coding-and-contrasts","dir":"Articles","previous_headings":"","what":"Coding and contrasts","title":"Factors, coding and contrasts","text":"Now. ’ve seen dummy coding simply using dummy numeric variables 0s 1s. fact, one way coding factors, one coding scheme.1 Different coding schemes R called contrasts. Dummy coding called treatment contrasts R.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/contrasts.html","id":"treatment-contrasts","dir":"Articles","previous_headings":"Coding and contrasts","what":"Treatment contrasts","title":"Factors, coding and contrasts","text":"term treatment contrasts comes clinical sciences test, example, efficacy medical intervention (drug, surgery, etc…) comparing control group (received “treatment”) group received medical intervention (treatment group). control group can used reference group see treatment group benefited medical treatment (.e. treatment group’s health improved intervention relative control group, one can infer treatment effective). Let’s look treatment contrasts R. previous section, ’ve illustrating dummy coding assigning 0s 1s using one dummy variables. practice, need run real analyses, R hood . Factors R coded treatment contrasts default. Also default, first level set reference level (order alphabetical default). reference level level gets coded 0s, seen dinosaur’s diet factor (carnivorous dummy_1 = 0 dummy_2 = 0). Let’s see example using data table measurements vowel duration. example, let’s take vowel column. column indicates vowel measurement taken , can //, /o/, /u/. convert vowel column factor, levels , o u, reference level. get sense factor coded treatment contrasts, can print dummy coding table contr.treatment() function. Now, let’s run regression model v1_duration outcome variable vowel predictor. summary returns three coefficients: Intercept. vowelo. vowelu. Since reference level vowel, Intercept corresponds mean duration vowel , .e. 128 ms. coefficient o difference mean duration o mean duration reference level (.e. Intercept). o 5.6 ms shorter average (shorter coefficient negative). Finally, coefficient u difference mean duration u mean duration reference level u 29.7 ms shorter . treatment contrasts work.","code":"data(\"vowels\") glimpse(vowels) #> Rows: 886 #> Columns: 9 #> $ item          <dbl> 20, 2, 11, 1, 15, 10, 13, 3, 14, 19, 4, 6, 16, 17, 5, 23… #> $ speaker       <chr> \"it01\", \"it01\", \"it01\", \"it01\", \"it01\", \"it01\", \"it01\", … #> $ word          <chr> \"pugu\", \"pada\", \"poco\", \"pata\", \"boco\", \"podo\", \"boto\", … #> $ v1_duration   <dbl> 95.23720, 138.96844, 126.93226, 127.49888, 132.33310, 12… #> $ c2_voicing    <chr> \"voiced\", \"voiced\", \"voiceless\", \"voiceless\", \"voiceless… #> $ vowel         <chr> \"u\", \"a\", \"o\", \"a\", \"o\", \"o\", \"o\", \"a\", \"o\", \"u\", \"a\", \"… #> $ c2_place      <chr> \"velar\", \"coronal\", \"velar\", \"coronal\", \"velar\", \"corona… #> $ speech_rate   <dbl> 4.893206, 5.015636, 4.819541, 5.031662, 5.063435, 5.0632… #> $ speech_rate_c <dbl> -0.55937531, -0.43694485, -0.63303978, -0.42091937, -0.3… vowels <- vowels %>%   mutate(vowel = as.factor(vowel))  levels(vowels$vowel) #> [1] \"a\" \"o\" \"u\" contr.treatment(levels(vowels$vowel)) #>   o u #> a 0 0 #> o 1 0 #> u 0 1 vow_lm <- lm(v1_duration ~ vowel, data = vowels)  summary(vow_lm) #>  #> Call: #> lm(formula = v1_duration ~ vowel, data = vowels) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -66.874 -22.567  -2.293  17.025 106.755  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  128.616      1.806  71.227   <2e-16 *** #> vowelo        -5.641      2.549  -2.213   0.0272 *   #> vowelu       -29.763      2.603 -11.432   <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 31.38 on 883 degrees of freedom #> Multiple R-squared:  0.1419, Adjusted R-squared:  0.1399  #> F-statistic: 72.99 on 2 and 883 DF,  p-value: < 2.2e-16"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/contrasts.html","id":"sum-contrasts","dir":"Articles","previous_headings":"Coding and contrasts","what":"Sum contrasts","title":"Factors, coding and contrasts","text":"Another type coding effect coding. R, corresponding contrast type -called sum contrasts. using sum contrasts, levels factor coded using 1s, -1s 0s. sum values dummy variable always get 0 (hence name “sum” contrast). Let’s see happens factor vowel using sum contrasts (remember factors use treatment contrasts default). sum coding look like factor: Since 3 levels, need two dummy variables. coded 1, 0, o coded 0, 1, u -1, -1. set contrasts factor sum coding, can run following: sum contrasts reference level fact grand mean. model vowel duration means Intercept coefficient grand mean vowel duration. Let’s rerun model look output. Intercept now 116 ms, mean mean vowel duration across three vowels 116 ms. can check taking mean: Yup, pretty close (small differences fine). now coefficients called vowel1 vowel2? , respectively, difference mean duration grand mean, difference mean duration o grand mean. 11.8 ms longer grand mean, o 6.1 ms longer grand mean. u ? Easy. just subtract coefficients o grand mean: \\(116.8 - 11.8 - 6.1 = 98.9\\). want check correct, mean duration u according model used treatment contrasts \\(128.616 - 29.763 = 98.853\\).","code":"contr.sum(levels(vowels$vowel)) #>   [,1] [,2] #> a    1    0 #> o    0    1 #> u   -1   -1 contrasts(vowels$vowel) <- \"contr.sum\" # If you want to change it back to treatment contrasts you can run # contrasts(vowels$vowel) <- \"contr.treatment\" vow_lm <- lm(v1_duration ~ vowel, data = vowels)  summary(vow_lm) #>  #> Call: #> lm(formula = v1_duration ~ vowel, data = vowels) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -66.874 -22.567  -2.293  17.025 106.755  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  116.815      1.055 110.728  < 2e-16 *** #> vowel1        11.801      1.483   7.957 5.40e-15 *** #> vowel2         6.160      1.481   4.160 3.49e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 31.38 on 883 degrees of freedom #> Multiple R-squared:  0.1419, Adjusted R-squared:  0.1399  #> F-statistic: 72.99 on 2 and 883 DF,  p-value: < 2.2e-16 mean(vowels$v1_duration) #> [1] 117.2747"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/contrasts.html","id":"sum-contrasts-and-interactions","dir":"Articles","previous_headings":"Coding and contrasts","what":"Sum contrasts and interactions","title":"Factors, coding and contrasts","text":"Sum contrasts can handy model contains interactions factors. Let’s say want include model vowel duration predictor specifies voicing stop following vowel. also add interaction vowel voicing, can model differences effect voicing across vowels. Now Intercept mean vowel duration following stop voiced (reference level c2_voicing). means average vowel followed voiced stop 123 ms long data. coefficient c2_voicingvoiceless tells us mean effect c2_voicing vowel duration, averaged across vowels. , average, vowel 13 ms shorter followed voiceless stop. coefficients vowel1 vowel2 indicate difference average vowel duration voiced stop (Intercept) o respectively. , get difference average vowel duration u voiceless stop mean vowel duration, just need subtract coefficients vowel1 vowel2 Intercept: \\(123.5 - 14.6 - 8.5 = 100.4\\). last two coefficients, c2_voicingvoiceless:vowel1 c2_voicingvoiceless:vowel2 correspond difference effect voicing average effect voicing (c2_voicingvoiceless, .e. -13 ms) effect voicing o respectively. , decrease vowel duration 5.6 ms greater average effect, decrease vowel duration o 4.8 ms greater average effect. Following usual formula, effect voicing u \\(-13.309 - (-5.6) - (-4.8) = -2.9\\).","code":"vow_lm_2 <- lm(v1_duration ~ c2_voicing + vowel + c2_voicing:vowel, data = vowels)  summary(vow_lm_2) #>  #> Call: #> lm(formula = v1_duration ~ c2_voicing + vowel + c2_voicing:vowel,  #>     data = vowels) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -64.905 -23.262  -2.033  18.134 115.813  #>  #> Coefficients: #>                            Estimate Std. Error t value Pr(>|t|)     #> (Intercept)                 123.469      1.449  85.232  < 2e-16 *** #> c2_voicingvoiceless         -13.309      2.049  -6.496 1.37e-10 *** #> vowel1                       14.606      2.037   7.171 1.57e-12 *** #> vowel2                        8.564      2.033   4.212 2.79e-05 *** #> c2_voicingvoiceless:vowel1   -5.609      2.880  -1.947   0.0518 .   #> c2_voicingvoiceless:vowel2   -4.808      2.876  -1.672   0.0949 .   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 30.47 on 880 degrees of freedom #> Multiple R-squared:  0.1937, Adjusted R-squared:  0.1891  #> F-statistic: 42.29 on 5 and 880 DF,  p-value: < 2.2e-16"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_03.html","id":"load-data","dir":"Articles","previous_headings":"","what":"Load data","title":"Application to regression II - Priors and Bayesian updating","text":"Let’s load packages data going use.","code":"# Packages library(brms) library(learnB4SS) library(extraDistr) library(tidyverse)  # Data data(\"polite\")"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_03.html","id":"which-priors","dir":"Articles","previous_headings":"","what":"Which priors?","title":"Application to regression II - Priors and Bayesian updating","text":"Depending type model running nature included predictors, specify certain set priors. can easily learn priors can specify model using function brms::get_prior(). try second. trying find kind effect politeness () articulation rate. first approximation, start following model: find priors available model, just need specify model formula data get_prior() function. Go ahead fill following code snippet run ’re done get list priors! ’ll learn interpret output next section.","code":"# Don't run me yet! m1 <- brm(   formula = articulation_rate ~ attitude,   data = polite ) get_prior(  formula = ...,  data = ... )"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_03.html","id":"understand-the-syntax","dir":"Articles","previous_headings":"","what":"Understand the syntax","title":"Application to regression II - Priors and Bayesian updating","text":"run get_prior(), table returned columns. row corresponds one prior (one model parameter). now, can focus just first three (prior, class, coef).","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_03.html","id":"prior","dir":"Articles","previous_headings":"Understand the syntax","what":"prior","title":"Application to regression II - Priors and Bayesian updating","text":"column prior tells prior probability distributions set default brms. model, first two default priors (flat), .e. uniform distributions (values equally probable). two priors Student-t distributions. (prior specification ). want change something appropriate (talk makes appropriate prior tomorrow).","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_03.html","id":"class","dir":"Articles","previous_headings":"Understand the syntax","what":"class","title":"Application to regression II - Priors and Bayesian updating","text":"columns class coef ones check. class indicates type model parameter: Intercept model intercept,b indicates predictor term, sigma model’s residual standard deviation.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_03.html","id":"coef","dir":"Articles","previous_headings":"Understand the syntax","what":"coef","title":"Application to regression II - Priors and Bayesian updating","text":"third column, coef indicates model predictor prior assigned. model , ’s one predictor (attitude), ’s can see get_prior() output. Since attitude two levels (inf pol), one parameter second level pol (level inf included Intercept).","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_03.html","id":"specify-your-priors","dir":"Articles","previous_headings":"","what":"Specify your priors","title":"Application to regression II - Priors and Bayesian updating","text":"’s quite easy specify priors brms. just need use function prior()! Since need specify one prior, need multiple calls prior(), concatenated c(). Like following example: Within function prior(), need specify prior distribution parameters, prior class (class), prior coefficient (coef) . simple model , need three priors: prior Intercept. prior slope attitudepol. prior standard deviation sigma. Remember get_prior() also lists class-generic priors (case, prior class = b), don’t need specify specify prior coefficient (case, class = b, coef = attitudepol). ’ll give head start telling priors can use model: Intercept: Normal distribution mean = 0 sd = 15. attitudepol: Normal distribution mean = 0 sd = 10. sigma: Half-Cauchy distribution mean = 0 sd = 1. Half Cauchy distribution something haven’t seen yet. distribution helpful estimating measures like standard deviations. ’s “half” positive values taken consideration, since standard deviation can ever positive. Half Cauchy distribution mean 0 standard deviation 1 looks like:  can see, probability getting bigger values drops quickly moving away 0. ensures prior standard deviation “regularising” effect (talk regularising priors tomorrow), helps brms estimate standard deviation. Now go ahead fill following code snippet right parameters arguments: now let’s fit model using priors!","code":"my_priors <- c(   prior(...),   prior(...),   prior(...) ) x <- seq(0, 10, by = 0.1) y <- dhcauchy(x, sigma = 1) ggplot() +   aes(x, y) +   geom_line(size = 2) +   labs(     x = \"Articulation rate (syl/s)\", y = \"Density\",     title = \"Half Cauchy distribution\",     subtitle = expression(paste(mu, \"=0\", \", \", sigma, \"=1\"))   ) #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated. my_priors <- c(   prior(normal(..., ...), class = Intercept),   prior(normal(..., ...), class = b, coef = ...),   prior(cauchy(0, ...), class = ...) ) b_mod_01 <- brm(   articulation_rate ~ attitude,   data = polite,   prior = my_priors )"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_06.html","id":"get-set-up","dir":"Articles","previous_headings":"","what":"Get set up","title":"Application to regression III - More Bayesian inference","text":"First, let’s load packages data using.","code":"# Packages library(\"learnB4SS\") library(\"dplyr\") library(\"ggplot2\") library(\"brms\") library(\"bayestestR\")  # Data data(polite) mod_00 <- system.file(\"extdata/b_mod_00.rds\", package = \"learnB4SS\") mod_01 <- system.file(\"extdata/b_mod_01.rds\", package = \"learnB4SS\")"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_06.html","id":"credible-intervals","dir":"Articles","previous_headings":"","what":"Credible intervals","title":"Application to regression III - More Bayesian inference","text":"first tool consider making statistical inferences Bayesian framework credible interval. heard Bayesian inference , good chance also heard credible intervals. describe Bayesian counterpart confidence intervals frequentist framework. need calculate credible interval probability distribution. R, can simple vector values. ’ll illustrate sampling fake data calculating CI using base R. can see 95% CI posterior -0.5639172, 10.6812157. can simplify process even using hdi function bayestestR package. benefits see bit. Notice ranges slightly different. HDI, highest density interval, special type credible interval. several methods calculating CI, won’t go now. purposes stick HDI. One advantage using hdi function bayestestR package also print methods. means can wrap plot() function around hdi object, like :   Beautiful! Now let’s repeat using real posterior model object. Recall fit intercept-model previously: Let’s get posterior distribution calculate HDI intercept. 95% certain value intercept falls 6.5 6.8. Cool. Let’s plot well.   Now repeat process yesterdays model included predictor attitude. model , just case: Let’s calculate CI parameter b_attitudepol. looks like 95% CI encompass 0. Let’s take look plot.   helpful. can see 95% HDI encompass 0, posterior mass positive (blue). words, positive values still possibility, given data, model, prior assumptions. Bonus: can also use hdi function directly model object. , like , can use built-print methods quickly generate plot:","code":"# Generate 'fake' posterior by sampling values from a normal distribution with  # mean of 5 and SD of 3 posterior <- rnorm(n = 1000, mean = 5, sd = 3)  # Use quantile function on posterior quantile(posterior, c(0.025, 0.975)) #>       2.5%      97.5%  #> -0.5639172 10.6812157 # bayesTestR::hdi hdi_ex1 <- hdi(posterior) hdi_ex1 #> 95% HDI: [-0.57, 10.68] # Generate a plot from the hdi object plot(hdi_ex1) # Intercept-only model b_mod_00 <- brm(articulation_rate ~ 1, data = polite) # Get posterior samples post_00 <- posterior_samples(b_mod_00)  # Calculate HDI hdi_ex2 <- hdi(post_00$b_Intercept) hdi_ex2 #> 95% HDI: [6.52, 6.83] # Plot it plot(hdi_ex2) # Specify priors priors_simple <- c(   prior(normal(0, 15), class = Intercept),   prior(normal(0, 10), class = b, coef = attitudepol),   prior(cauchy(0, 1), class = sigma) )  # Fit model b_mod_01 <- brm(   formula = articulation_rate ~ attitude,   data = polite,   prior = priors_simple )  # Get posterior samples post_01 <- posterior_samples(b_mod_01) # Calculate HDI hdi_ex3 <- hdi(post_01$b_attitudepol) hdi_ex3 #> 95% HDI: [-0.71, -0.13] # Plot it plot(hdi_ex3) # Get HDI of all paramters of model hdi(b_mod_01) #> Highest Density Interval #>  #> Parameter   |        95% HDI #> ---------------------------- #> (Intercept) | [ 6.68,  7.09] #> attitudepol | [-0.71, -0.13] # Assign hdi to object and plot hdi_on_mod <- hdi(b_mod_01) plot(hdi_on_mod, show_intercept = T)"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_06.html","id":"probability-of-direction","dir":"Articles","previous_headings":"","what":"Probability of direction","title":"Application to regression III - More Bayesian inference","text":"Another tool decision making probability direction, also known maximum probability effect. probability direction tells us proportion posterior sign (, + -) median distribution. value can range 50% 100% (0.5 - 1.0). calculate probability direction resort bayestestR package use p_direction function. function calculate probability direction vector, posterior samples taken model object, directly model object: , can take advantage print methods quickly generate plot:   build intuition probability direction corresponds frequentist p-value can use pd_to_p p_to_pd functions convert two (recommended). plot shows linear relationship probability direction frequentist p-value.","code":"# \"Fake\" posterior example  p_direction(posterior) #> Probability of Direction: 0.96 # PD of intercept-only model p_direction(post_00$b_Intercept) #> Probability of Direction: 1.00 # PD for all parameters in b_mod_01 p_direction(b_mod_01) #> Probability of Direction #>  #> Parameter   |     pd #> -------------------- #> (Intercept) |   100% #> attitudepol | 99.70% # Assign PD to object and print pd_on_mod <- p_direction(b_mod_01) plot(pd_on_mod, show_intercept = T) # Calculate p-value from PD pd_to_p(0.987) #> [1] 0.026  # Calculate PD from p-value p_to_pd(0.01) #> [1] 0.995"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_06.html","id":"ropes","dir":"Articles","previous_headings":"","what":"ROPEs","title":"Application to regression III - More Bayesian inference","text":"next tool going talk combines highest density credible interval region practical equivalence, ROPE can use ROPE calculate proportion HDI posterior distribution lies within bounds. ROPEs useful allow researcher define range values consider practically equivalent null effect use rope function basyestestR package. default, rope function calculate proportion 95% CI falls within ROPE range. Like , can operate single distribution stored vector can set range ROPE size HDI using arguments range ci, respectively. default sets CI 95% ROPE range ±0.1, unless input Bayesian model. case, function rope_range used calculate ROPE (following Kruschke 2018, see ?rope_range). \\[\\color{black}{\\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{\\frac{\\sigma^2_{1} + \\sigma^2_{2}}{2}}}}\\] also print methods rope objects.   can also use ROPE multiple CIs.","code":"# Calculate % of HDI within rope using simulated data rope(posterior) #> # Proportion of samples inside the ROPE [-0.10, 0.10]: #>  #> inside ROPE #> ----------- #> 0.74 % # Calculate % of HDI within rope w/ specific settings rope(posterior, range = c(-0.1, 0.1), ci = 0.99) #> # Proportion of samples inside the ROPE [-0.10, 0.10]: #>  #> inside ROPE #> ----------- #> 0.71 % # Calculate an appropriate ROPE following Kruschke 2018 rope_range(b_mod_01) #> [1] -0.1164812  0.1164812  # Run it on our model rope_on_mod01 <- rope(b_mod_01, range = c(-0.2, 0.2), ci = 0.95) rope_on_mod01 #> # Proportion of samples inside the ROPE [-0.20, 0.20]: #>  #> Parameter   | inside ROPE #> ------------------------- #> Intercept   |      0.00 % #> attitudepol |      5.79 % plot(rope_on_mod01, show_intercept = T) # Set more strict ROPE and several HDIs rope_on_mod02 <- rope(b_mod_01, range = c(-0.2, 0.2), ci = c(0.9, 0.95, 0.99)) rope_on_mod02 #> # Proportions of samples inside the ROPE [-0.20, 0.20]: #>  #> ROPE for the 90% HDI: #>  #> Parameter   | inside ROPE #> ------------------------- #> Intercept   |      0.00 % #> attitudepol |      3.33 % #>  #>  #> ROPE for the 95% HDI: #>  #> Parameter   | inside ROPE #> ------------------------- #> Intercept   |      0.00 % #> attitudepol |      5.79 % #>  #>  #> ROPE for the 99% HDI: #>  #> Parameter   | inside ROPE #> ------------------------- #> Intercept   |      0.00 % #> attitudepol |      7.58 % plot(rope_on_mod02)"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_06.html","id":"your-turn","dir":"Articles","previous_headings":"","what":"Your turn!","title":"Application to regression III - More Bayesian inference","text":"Now let’s take tools use test different hypotheses using polite dataset. Let’s fit following model: articulation_rate ~ musicstudent simplify, can use priors . hint, need fill details. Using credible interval, determine whether compelling evidence articulation rate non-music students music students non-zero. probability estimate difference non-music students music students, .e., musicstudentyes, negative? , consider non-music/music student difference, quantify uncertainty using ROPE (use range CI width choice). Plot result. Hint: can calculate metrics single function! Try describe_posterior.","code":"# Specify priors priors_simple <- c(   prior(normal(0, 15), class = Intercept),   prior(normal(0, 10), class = b),   prior(cauchy(0, 1), class = sigma) )  b_mod_02 <- brm(   formula = ???,    prior = ???,    data = polite ) describe_posterior(b_mod_02, rope_range = c(-0.15, 0.15), ci = 0.99)"},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_08.html","id":"introduction","dir":"Articles","previous_headings":"Walkthrough","what":"Introduction","title":"Application to regression IV - Leveling up to hierarchical models","text":"now, dealt simple models, just particularly relevant actually research, right? section, level . add additional parameters regression form fixed effects random effect. Sounds much like stuff want , right? going estimate parameters specify appropriate priors . session much closer operational Bayesian actual research.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_08.html","id":"a-simple-model","dir":"Articles","previous_headings":"Walkthrough","what":"A simple model","title":"Application to regression IV - Leveling up to hierarchical models","text":"Okay let’s think . model far looked alongside priors: Brms uses gaussian link function default (made explicit ), .e. assume model residuals normally distributed. really case? measurement bound 0, can’t negative values articulation rate, right? likely articulate rate data skewed right, values can vary toward higher values toward lower values. Brms allows us critically evaluate model fit comparing model using -called posterior predictive checks. Posterior predictive checks , simple words, “simulating replicated data fitted model comparing observed data” (Gelman & Hill 2007: 158). , use posterior predictive checks investigate whether systematic discrepancies observed simulated data. Looking plot, see thick dark blue line data, bunch light blue lines simulated data based model. model assumes normal distribution, posterior draws much symmetric actual data, thus notice discrepancy model data. situations common measurements bound zero (e.g. duration response latency). account , can use different link function (specified family argument). appropriate model use log normal distribution articulation rate. change family, model fit measurement log space, change priors. Let’s throw numbers run prior_predictive check, .e. running model prior information (sample_prior = \"\"). also need specify lognormal family running model. Well, priors weakly informative cover consider possible range articulation rate. However constraints parameter space substantially already. Good enough. Let’s refit model specify lognormal family. check model fit : Much better, right? critically evaluating model assumptions actual data discovered lognormal distribution much better assumption underlying generative model (.e. data came ). Fantastic! Let’s look results (remember results now log transformed) want plot something traditional, .e. posterior values polite informal speech productions, can use neat functions packages. way can plot data log space compare two conditions immediately.","code":"# specify priors priors_simple <- c(   # prior for the Intercept (= the reference level)   prior(normal(0, 15), class = Intercept),   # prior for the fixed effect coefficient for polite attitude   prior(normal(0, 10), class = b, coef = attitudepol),   # prior for the residual variance   prior(cauchy(0, 1), class = sigma) ) # specify model simple_model <- brm(   articulation_rate ~ attitude,     data = polite,   prior = priors_simple,   family = gaussian ) # posterior predictive check pp_check(simple_model, nsamples = 100) # specify priors in log space priors_simple_log <- c(   # prior for the Intercept (= the reference level)   prior(normal(0, 2), class = Intercept),   # prior for the fixed effect coefficient for polite attitude   prior(normal(0, .25), class = b, coef = attitudepol),   # prior for the residual variance   prior(cauchy(0, 0.1), class = sigma) ) # specify model simple_model2_prior <- brm(   articulation_rate ~      attitude,       data = polite,     prior = priors_simple_log,     # specify that the model should only estimate the posterior based on the information in the prior     sample_prior = \"only\",     # specify lognormal family function     family = lognormal   ) # quick and dirty plot on the original scale conditional_effects(simple_model2_prior) # specify model simple_model2 <- brm(   articulation_rate ~      attitude,       data = polite,     prior = priors_simple_log,     family = lognormal   ) # posterior predictive check pp_check(simple_model2, nsamples = 100) # Extract posterior coefficient and plot simple_model2 %>%   spread_draws(b_attitudepol) %>%    # plot   ggplot(aes(x = b_attitudepol)) +   stat_halfeye() +   geom_vline(xintercept = 0, lty = \"dashed\") +   labs(x = \"log(articulation rate)\") # Extract posterior and plot predicted values for both levels polite %>%    data_grid(attitude) %>%   add_predicted_draws(simple_model2) %>%   # plot   ggplot(aes(x = .prediction, y = attitude, fill = attitude)) +   stat_halfeye() +   scale_fill_manual(values = c(\"#8970FF\", \"#FFA70B\")) +    labs(x = \"articulation rate\")"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_08.html","id":"adding-predictors","dir":"Articles","previous_headings":"Walkthrough","what":"Adding predictors","title":"Application to regression IV - Leveling up to hierarchical models","text":"model estimates effect attitude articulation rate (articulation_rate). ... also want estimate differences attitude across different experimental tasks? Let’s add task predictor add prior well. Let’s stick weakly informative priors , centered zero (expect difference articulation rate different tasks acknowledge possibility variance around value). Looking summary model, get table two coefficients population parameters attitudepol tasknot. Let’s remind reflect change articulation rate reference level (attitude = informal (inf), task = dialog completion task (dct)) attitude = polite (attitudepol) task = note (tasknot), respectively. can inspect posteriors predictors just like : posterior distributions clearly located left zero, suggesting attitude task affect articulation-rate extend (given model, data, prior assumptions)","code":"# get prior get_prior(  formula = articulation_rate ~ attitude + task,   data = polite, family = lognormal ) # specify priors priors_multiple <- c(   prior(normal(0, 2), class = Intercept),   prior(normal(0, .25), class = b, coef = attitudepol),   prior(normal(0, .25), class = b, coef = tasknot),   prior(cauchy(0, .1), class = sigma) ) # tun model multiple_model <- brm(   articulation_rate ~ attitude + task,   data = polite,   prior = priors_multiple,   family = lognormal)  summary(multiple_model) # Extract posterior coefficient attitudepol post_multiple_attitude <- multiple_model %>%   spread_draws(b_attitudepol, b_tasknot)  ggplot(post_multiple_attitude) +    stat_halfeye(aes(x = b_attitudepol)) +   labs(title = \"posterior for effect of attitude\",          x = \"log(articulation rate)\") +   geom_vline(xintercept = 0, lty = \"dashed\")   # Extract posterior coefficient tasknot  ggplot(post_multiple_attitude) +    stat_halfeye(aes(x = b_tasknot)) +   labs(title = \"posterior for effect of task\",          x = \"log(articulation rate)\") +   geom_vline(xintercept = 0, lty = \"dashed\")"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_08.html","id":"adding-random-effects","dir":"Articles","previous_headings":"Walkthrough","what":"Adding random effects","title":"Application to regression IV - Leveling up to hierarchical models","text":"running Bayesian regression models really much different running frequentist regression models. Practically, difference far specify prior knowledge. Conceptually, obviously interpret output differently terms inference draw, business usual . Now, probably suspect model quite appropriate data, right? take account fact data points independent . multiple speakers produced multiple data points, observations speakers independent need take account robust inference. let’s also add appropriate random effect structure, including -subject random intercept well -subject random slope attitude. can write code run model easily, familiar lme4 syntax. brms syntax handles random effect structure exactly like lme4 like : Looks familiar? priors? need specify priors new elements well? ? Generally, encourage specify priors elements model, let us try well. need specify four parameters Group-Level Effects. Three variance components specify half-cauchy distributions, just like residual variance. final parameter correlation coefficient group level effects specified using Lewandowski-Kurowicka-Joe prior (LKJ). . Now can run model. add one new argument brm() function: seed argument allows us set random seed. sampling procedure random initial state, run model machine, get slightly different results someone else. seed can fix initial state obtain exact results. great, can make results fully reproducible way. Let us walk summary . now one part table called “Group-Level Effects”. part table gives us models estimates four parameters: much subjects’ baseline varies (sd(Intercept)), much vary informal condition (sd(attitudeinf)), much subjects vary polite condition (sd(attitudepol)), correlation sd(attitudeinf) sd(attitudepol) . , parameters, receive Estimate mean posterior distribution range plausible values within 95% Credible Interval (l-95% CI - u-95% CI). Except correlation, estimates bound 0, .e. variance can positive. correlation coefficient can vary -1 1. second part summary table, .e. Population-Level Effects, change summarizes regression coefficients fixed effects. Just like . go. run linear mixed effects model within Bayesian framework. specified priors random fixed effects. left interpreting output.","code":"# model specification complex_model_formula =    articulation_rate ~ attitude + task +     # add random intercept     (1 | subject) +     # add random slope     (0 + attitude | subject) # get prior get_prior(   articulation_rate ~ attitude + task +     (1 | subject) +     (0 + attitude | subject),      data = polite,    family = lognormal ) # specify priors priors_complex <- c(   prior(normal(0, 2), class = Intercept),   prior(normal(0, 0.25), class = b, coef = attitudepol),   prior(normal(0, 0.25), class = b, coef = tasknot),   # specify weakly informative prior for the random effects (slopes)   prior(cauchy(0, 0.1), class = sd, coef = Intercept, group = subject),   prior(cauchy(0, 0.1), class = sd, coef = attitudeinf, group = subject),   prior(cauchy(0, 0.1), class = sd, coef = attitudepol, group = subject),   # specify weakly informative prior for the correlation between random intercept and slope   prior(lkj(2), class = cor, group = subject),   prior(cauchy(0, 0.1), class = sigma) ) # Run model complex_model <- brm(   articulation_rate ~ attitude + task +     # add random intercept     (1 | subject) +     # add random slope     (0 + attitude | subject),      data = polite,    prior = priors_complex,    family = lognormal,   # set seed    seed = 999 )  summary(complex_model) # Extract posterior coefficient politeness post_complex <- complex_model %>%   spread_draws(b_attitudepol, b_tasknot)   # plot ggplot(post_complex) +    stat_halfeye(aes(x = b_attitudepol))   labs(x = \"log(articulation rate)\") +   geom_vline(xintercept = 0, lty = \"dashed\")"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_08.html","id":"making-inference","dir":"Articles","previous_headings":"Walkthrough","what":"Making inference","title":"Application to regression IV - Leveling up to hierarchical models","text":"Now let’s use posteriors draw probability inferences. 95% credible interval attitudepol coefficient probability direction? Fantastic! best estimate effect attitude 95% CrI [-0.12, -0.01] 0.98 probability negative. Although majority plausible values negative, positive values still plausible (given data, model, priors).","code":"# extract 95% HDI hdi(post_complex$b_attitudepol) cat(\"\\n\") # extract probability of direction p_direction(post_complex$b_attitudepol)"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/ex_08.html","id":"exercise","dir":"Articles","previous_headings":"","what":"Exercise","title":"Application to regression IV - Leveling up to hierarchical models","text":"Now ’s turn: goal run following model: f0mn ~ attitude + (1 | subject) + (0 + attitude | subject) First, see priors need specify using get_prior() function. (unsure, see code examples ) Now specify weakly informative priors parameters. intercept, chose normal prior mean = 0 sd = 500. simplicity sake, let us just go normally distributed priors parameters (mean = 0, sd = 100) Now run model specified priors. Use seed = 1111 order get results. Extract posteriors coefficient attitudepol plot . Now let’s use posteriors draw probability inferences. 95% credible interval attitudepol coefficient probability direction? hold . model fit okay? Critically check model fit pp_check(). think model priors capture generative process appropriately? might issue? Can fix ?","code":"# get prior get_prior(  formula = ...,  data = ... ) # specify priors priors_ex <- c(   ... ) # Run model ex_model <- brm(   formular = ...,   data = ...,   prior = ...,   seed = 1111 ) # Extract posterior coefficient politeness ex_post <- ex_model %>%   spread_draws(...)  # plot ggplot(...) +   ... # extract 95% HDI hdi(...)  # extract probability of direction p_direction(...)"},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/articles/faqs.html","id":"do-i-need-to-know-how-to-use-r-in-order-to-participate","dir":"Articles","previous_headings":"1. General","what":"Do I need to know how to use R in order to participate?","title":"FAQs","text":"Yes. guide process running Bayesian analysis lot examples, won’t go basics R. wish brush R skills, recommend interactive tool swiRl, can find : http://swirlstats.com/.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/faqs.html","id":"do-i-need-to-know-basic-statistics-in-order-to-participate","dir":"Articles","previous_headings":"1. General","what":"Do I need to know basic statistics in order to participate?","title":"FAQs","text":"Yes. give necessary notions understand perform Bayesian analysis, revise concepts non-Bayesian statistics, expect familiarity linear regressions. particular, won’t cover topics interpreting regression coefficients, understanding interactions, including random effects, coding factors. easy--follow introduction, highly recommend Bodo Winter’s book Statistics linguists: introduction using R (also includes chapter basic R).","code":""},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/articles/faqs.html","id":"i-live-in-an-inconvenient-time-zone--will-i-be-able-to-attend-at-a-different-time","dir":"Articles","previous_headings":"2. Practicalities","what":"I live in an inconvenient time-zone. Will I be able to attend at a different time?","title":"FAQs","text":"sorry accommodate time-zones, provide captioned recordings can watch follow along. record live sessions send link download recordings registrants one two days first two days. (won’t records third day, since 1:1 meetings).","code":""},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/articles/faqs.html","id":"on-macos-i-get-errors-related-to-the-command-line-tools--what-can-i-do","dir":"Articles","previous_headings":"3. Software","what":"On macOS, I get errors related to the Command Line Tools. What can I do?","title":"FAQs","text":"following might work (guarantee). Open Terminal run following code. recently updated macOS, update CLTs easiest way run Terminal:","code":"sudo rm -rf /Library/Developer/CommandLineTools sudo rm -rf /usr/local/include xcode-select --install sudo rm -rf /Library/Developer/CommandLineTools xcode-select --install"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/faqs.html","id":"on-linuxubuntu-i-get-dependency-issues-when-installing-rstan-","dir":"Articles","previous_headings":"3. Software","what":"On Linux/Ubuntu I get dependency issues when installing Rstan.","title":"FAQs","text":"users reported solution worked. Also using rrutter 4.0 instead rrutter 3.5 seems help.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"study-overview-and-data","dir":"Articles","previous_headings":"","what":"Study overview and data","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"","code":"glimpse(incomplete) #> Rows: 6,144 #> Columns: 8 #> $ order           <dbl> 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,… #> $ speaker_voice   <chr> \"VP11\", \"VP10\", \"VP07\", \"VP15\", \"VP06\", \"VP09\", \"VP08\"… #> $ item_pair       <dbl> 19, 11, 14, 23, 7, 15, 3, 13, 24, 2, 5, 20, 3, 8, 4, 9… #> $ RT              <dbl> 1531, 1009, 633, 1852, 3606, 4493, 15186, 1995, 940, 4… #> $ correct         <dbl> 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, … #> $ correct_voicing <chr> \"voiceless\", \"voiceless\", \"voiced\", \"voiced\", \"voicele… #> $ listener        <chr> \"L01\", \"L01\", \"L01\", \"L01\", \"L01\", \"L01\", \"L01\", \"L01\"… #> $ repetitiontype  <chr> \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", …"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"model-formula","dir":"Articles","previous_headings":"","what":"Model formula","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"","code":"m1_bf <- brmsformula(   correct ~     correct_voicing *     repetitiontype +     # random slopes for interaction across listeners     (correct_voicing * repetitiontype | listener) +     # random slopes for interaction across speaker voices     (correct_voicing * repetitiontype | speaker_voice) +     # random slopes for interaction across minimal pairs     (correct_voicing * repetitiontype | item_pair),   family = bernoulli() )"},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"prior-distribution-of-the-outcome-variable-likelihood-family","dir":"Articles","previous_headings":"Priors and prior predictive checks","what":"Prior distribution of the outcome variable (likelihood, family)","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"\\[correct_i \\sim Bernoulli(p)\\]","code":"y <- dbern(c(0, 1), p = 0.75)  ggplot() +   aes(c(\"0\", \"1\"), y) +   geom_linerange(aes(ymin = 0, ymax = y), size = 2, colour = b4ss_colors) +   geom_point(size = 5, colour = b4ss_colors) +   ylim(0, 1) +   labs(x = \"outcome\", y = \"p\") #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated."},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"get-priors","dir":"Articles","previous_headings":"Priors and prior predictive checks","what":"Get priors","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"","code":"# get_prior(m1_bf, data = incomplete) get_prior(m1_bf, data = incomplete) %>%   as_tibble() %>%   select(prior:group) #> # A tibble: 25 × 4 #>    prior                  class     coef                                   group #>    <chr>                  <chr>     <chr>                                  <chr> #>  1 \"\"                     b         \"\"                                     \"\"    #>  2 \"\"                     b         \"correct_voicingvoiceless\"             \"\"    #>  3 \"\"                     b         \"correct_voicingvoiceless:repetitiont… \"\"    #>  4 \"\"                     b         \"repetitiontyperepeated\"               \"\"    #>  5 \"lkj(1)\"               cor       \"\"                                     \"\"    #>  6 \"\"                     cor       \"\"                                     \"ite… #>  7 \"\"                     cor       \"\"                                     \"lis… #>  8 \"\"                     cor       \"\"                                     \"spe… #>  9 \"student_t(3, 0, 2.5)\" Intercept \"\"                                     \"\"    #> 10 \"student_t(3, 0, 2.5)\" sd        \"\"                                     \"\"    #> # ℹ 15 more rows"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"log-odds-space","dir":"Articles","previous_headings":"Priors and prior predictive checks","what":"Log-odds space","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"","code":"dots <- tibble(   p = seq(0.1, 0.9, by = 0.1),   log_odds = qlogis(p) )  tibble(   p = seq(0, 1, by = 0.001),   log_odds = qlogis(p) ) %>%   ggplot(aes(p, log_odds)) +   geom_hline(yintercept = 0, alpha = 0.5) +   geom_vline(xintercept = 0.5, linetype = \"dashed\") +   geom_line(size = 2, colour = b4ss_colors[2]) +   geom_point(data = dots, size = 4, colour = b4ss_colors[1]) +   scale_x_continuous(breaks = seq(0, 1, by = 0.1), minor_breaks = NULL) +   scale_y_continuous(breaks = seq(-6, 6, by = 2), minor_breaks = NULL) +   labs(     title = \"Correspondence between probabilities and log-odds\",     x = \"probability\",     y = \"log-odds\"   ) # log-odds = 0; get probability glue::glue(\"Log-odds 0 = p(\", plogis(0), \")\") #> Log-odds 0 = p(0.5) # p = 0.5; get log-odds glue::glue(\"p(0.5) = log-odds \", qlogis(0.5)) #> p(0.5) = log-odds 0  cat(\"\\n\\n\")  # log-odds = 1.5; get probability glue::glue(\"Log-odds 1.5 = p(\", round(plogis(1.5), 4), \")\") #> Log-odds 1.5 = p(0.8176) # p = 0.5; get log-odds glue::glue(\"p(0.8176) = log-odds \", round(qlogis(0.8175745), 4)) #> p(0.8176) = log-odds 1.5"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"prior-for-the-intercept","dir":"Articles","previous_headings":"Priors and prior predictive checks","what":"Prior for the intercept","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"","code":"# Log-odds to p plogis(-6) # ~= 0  #> [1] 0.002472623 plogis(6) # ~= 1 #> [1] 0.9975274 x <- seq(-15, 15, by = 0.1) # 95% Cri [-6, +6] => SD = 6/2 = 3 y <- dnorm(x, mean = 0, sd = 3)  ggplot() +   aes(x, y) +   geom_line(size = 1.5, colour = b4ss_colors[1]) +   labs(x = \"log-odds\")"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"prior-for-b","dir":"Articles","previous_headings":"Priors and prior predictive checks","what":"Prior for b","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"","code":"# Log-odds to odds exp(-2) # ~= 0 #> [1] 0.1353353 exp(2) # ~= 7 #> [1] 7.389056 x <- seq(-4, 4, by = 0.1) # 95% Cri [-2, +2] => SD = 2/2 = 1 y <- dnorm(x, mean = 0, sd = 1)  ggplot() +   aes(x, y) +   geom_line(size = 1.5, colour = b4ss_colors[1]) +   labs(x = \"log-odds\")"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"prior-for-sd","dir":"Articles","previous_headings":"Priors and prior predictive checks","what":"Prior for sd","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"","code":"inverseCDF(c(0.025, 0.975), phcauchy, sigma = 0.1) #> [1] 0.003930135 2.545175934 x <- seq(0, 3, by = 0.01) y <- dhcauchy(x, sigma = 0.1)  ggplot() +   aes(x, y) +   geom_line(size = 1, colour = b4ss_colors[2]) +   labs(x = \"log-odds\")"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"set-priors","dir":"Articles","previous_headings":"Priors and prior predictive checks","what":"Set priors","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"","code":"priors <- c(   prior(normal(0, 3), class = Intercept),   prior(normal(0, 1), class = b),   prior(cauchy(0, 0.1), class = sd),   prior(lkj(2), class = cor) )"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"prior-predictive-checks","dir":"Articles","previous_headings":"Priors and prior predictive checks","what":"Prior predictive checks","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"","code":"m1_priorpc <- brm(   m1_bf,   data = incomplete,   prior = priors,   sample_prior = \"only\",   file = system.file(\"extdata/m1_priorpc.rds\", package = \"learnB4SS\") ) conditional_effects(m1_priorpc, effects = \"correct_voicing:repetitiontype\")"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"lets-try-very-strong-priors-for-comparison","dir":"Articles","previous_headings":"Priors and prior predictive checks","what":"Let’s try VERY strong priors for comparison","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"","code":"priors_strong <- c(   prior(normal(2, 0.1), class = Intercept),   prior(normal(1, 0.1), class = b),   prior(cauchy(0, 0.1), class = sd),   prior(lkj(2), class = cor) ) m1_priorpc_strong <- brm(   m1_bf,   data = incomplete,   prior = priors_strong,   sample_prior = \"only\",   cores = 4,   file = system.file(\"extdata/m1_priorpc_strong.rds\", package = \"learnB4SS\") ) conditional_effects(m1_priorpc_strong, effects = \"correct_voicing:repetitiontype\")"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"run-model","dir":"Articles","previous_headings":"","what":"Run model","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"","code":"m1_full <- brm(   m1_bf,   data = incomplete,   prior = priors,   cores = parallel::detectCores(),    chains = 4,    iter = 2000,    warmup = 1000,    file = system.file(\"extdata/m1_full.rds\", package = \"learnB4SS\") )"},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"checks-and-diagnostics","dir":"Articles","previous_headings":"Model output and interpretation","what":"Checks and diagnostics","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"","code":"# Sample data from the generative model and compare with real data pp_check(m1_full, nsamples = 200) #> Warning: Argument 'nsamples' is deprecated. Please use argument 'ndraws' #> instead. # Generate trace and density plots for MCMC samples # Only looking at pop-level parameters (no SDs or cor) plot(m1_full, pars = \"^b_\") #> Warning: Argument 'pars' is deprecated. Please use 'variable' instead. # Pairs plots to help spot degeneracies (doesn't apply here but  # useful if you have divergent transitions) pairs(m1_full, pars = \"^b_\") #> Warning: Argument 'pars' is deprecated. Please use 'variable' instead. # Check Rhat and ESS (remove the rest of the output so it doesn't distract) summary(m1_full)$fixed[, 5:7] #>                                                      Rhat  Bulk_ESS Tail_ESS #> Intercept                                       1.0049420  977.8247 1358.471 #> correct_voicingvoiceless                        1.0045544 1226.8754 1982.232 #> repetitiontyperepeated                          1.0013897 3884.5014 2915.458 #> correct_voicingvoiceless:repetitiontyperepeated 0.9999374 4262.5928 2502.050"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"prior-sensitivity-analysis","dir":"Articles","previous_headings":"Model output and interpretation","what":"Prior sensitivity analysis","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"","code":"m1_fixed <- tidy(m1_full, effects = \"fixed\", conf.level = 0.95, fix.intercept = FALSE) %>%   mutate(     ci_width = abs(conf.low - conf.high)   ) #> Warning in tidy.brmsfit(m1_full, effects = \"fixed\", conf.level = 0.95, #> fix.intercept = FALSE): some parameter names contain underscores: term naming #> may be unreliable! m1_fixed #> # A tibble: 4 × 8 #>   effect component term           estimate std.error conf.low conf.high ci_width #>   <chr>  <chr>     <chr>             <dbl>     <dbl>    <dbl>     <dbl>    <dbl> #> 1 fixed  cond      Intercept        0.0125    0.231   -0.437     0.477     0.914 #> 2 fixed  cond      correct_voici…   0.439     0.461   -0.454     1.33      1.79  #> 3 fixed  cond      repetitiontyp…   0.156     0.0938  -0.0272    0.336     0.363 #> 4 fixed  cond      correct_voici…  -0.224     0.133   -0.486     0.0319    0.518 m1_fixed %>%   mutate(     theta = c(0, 0, 0, 0),     sigma_prior = c(3, 1, 1, 1),     z = abs((estimate - theta) / std.error), # it's called here std.error but is the standard deviation     s = 1 - (std.error^2 / sigma_prior^2)   ) %>%   ggplot(aes(s, z, label = term)) +   geom_point() +   geom_label_repel(arrow = arrow()) +   xlim(0, 1) + ylim(0, 5) labels <- tibble(   x = c(0.25, 0.25, 0.6, 0.75),   y = c(1.25, 3.75, 1.25, 3.75),   labs = c(\"Poorly identified\", \"Prior/Posterior\\nconflict\", \"Ideal\", \"Overfit\") )  m1_fixed %>%   mutate(     theta = c(0, 0, 0, 0),     sigma_prior = c(3, 1, 1, 1),     z = abs((estimate - theta) / std.error), # it's called here std.error but is the standard deviation     s = 1 - (std.error^2 / sigma_prior^2)   ) %>%   ggplot(aes(s, z, label = term)) +   annotate(\"rect\", xmin = 0, ymin = 0, xmax = 0.5, ymax = 2.5, alpha = 0.5, fill = \"#e66101\") +   annotate(\"rect\", xmin = 0, ymin = 2.5, xmax = 0.5, ymax = 5, alpha = 0.5, fill = \"#fdb863\") +   annotate(\"rect\", xmin = 0.5, ymin = 0, xmax = 1, ymax = 2.5, alpha = 0.5, fill = \"#b2abd2\") +   annotate(\"rect\", xmin = 0.5, ymin = 2.5, xmax = 1, ymax = 5, alpha = 0.5, fill = \"#5e3c99\") +   geom_label(data = labels, aes(x, y, label = labs), colour = \"white\", fill = \"black\") +   geom_label_repel(fill = NA) +   geom_point() +   xlim(0, 1) + ylim(0, 5)"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"visual-effects","dir":"Articles","previous_headings":"","what":"Visual effects","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"quick dirty way assess posterior predictions using conditional_effects() function. also useful plots data original scale.  can also plot posterior distributions population-level coefficients. can conveniently done bayesplot package.  traditional plotting actual levels predictors, can use data_grid() add_fitted_draws() functions. example.  want add actual aggregated accuracy listener plot (example), can add information top. Makes informative plot.","code":"# quick and dirty plot on the original scale plot(conditional_effects(m1_full), ask = FALSE) posterior <- as.matrix(m1_full)  mcmc_areas(posterior,            pars = c(\"b_Intercept\",                      \"b_correct_voicingvoiceless\",                      \"b_repetitiontyperepeated\",                      \"b_correct_voicingvoiceless:repetitiontyperepeated\"),            # arbitrary threshold for shading probability mass            prob = 0.83) post_data <- incomplete %>%    data_grid(correct_voicing, repetitiontype) %>%   add_fitted_draws(m1_full, n = 4000, re_formula = NA) #> Warning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing. #> - Use [add_]epred_draws() to get the expectation of the posterior predictive. #> - Use [add_]linpred_draws() to get the distribution of the linear predictor. #> - For example, you used [add_]fitted_draws(..., scale = \"response\"), which #>   means you most likely want [add_]epred_draws(...). #> NOTE: When updating to the new functions, note that the `model` parameter is now #>   named `object` and the `n` parameter is now named `ndraws`.  post_plot <- post_data %>%   # plot   ggplot(aes(y = .value, x = correct_voicing,               fill = correct_voicing)) +   # density plus CrIs   stat_halfeye() +   # reference line at chance level = 0.5   geom_hline(yintercept = 0.5, lty = \"dashed\") +   # split by repetition type   facet_grid(~repetitiontype) +   # color code   scale_fill_manual(values = c(\"#8970FF\", \"#FFA70B\")) +    #scale_color_manual(values = c(\"#8970FF\", \"#FFA70B\")) +   # rename y axis   labs(y = \"probability of correct\",        x = \"underlying voicing\") post_plot #aggregate incomplete_agg <- incomplete %>%    group_by(listener, correct_voicing, repetitiontype) %>%    summarise(.value = mean(as.numeric(correct)), .groups = \"drop\")  post_plot +   geom_point(data = incomplete_agg,              aes(y = .value,                   x = correct_voicing,                   fill = correct_voicing),              pch = 21, alpha = 0.5,              position = position_nudge(x = -0.1))"},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/articles/full-analysis.html","id":"interpreting-model-output","dir":"Articles","previous_headings":"Inference","what":"Interpreting model output","title":"Fully worked-out analysis using a Bayesian regression with brms","text":"","code":"# Describe posterior using fixef (to get simple, clean output...  # focus on understanding each estimate in context and examining CrI's) # Use in conjuction with levels_plot-2 fixef(m1_full) #>                                                    Estimate  Est.Error #> Intercept                                        0.01248959 0.23130872 #> correct_voicingvoiceless                         0.43899959 0.46065151 #> repetitiontyperepeated                           0.15628813 0.09375277 #> correct_voicingvoiceless:repetitiontyperepeated -0.22414533 0.13267918 #>                                                        Q2.5      Q97.5 #> Intercept                                       -0.43687340 0.47723123 #> correct_voicingvoiceless                        -0.45357875 1.33148511 #> repetitiontyperepeated                          -0.02722672 0.33584627 #> correct_voicingvoiceless:repetitiontyperepeated -0.48584540 0.03186847 # Focus on describing posterior hdi_range <- bayestestR::hdi(m1_full, ci = c(0.65, 0.70, 0.80, 0.89, 0.95)) hdi_range #> Highest Density Interval #>  #> Parameter                                       |        65% HDI |        70% HDI |        80% HDI |        89% HDI |       95% HDI #> ----------------------------------------------------------------------------------------------------------------------------------- #> (Intercept)                                     | [-0.21,  0.21] | [-0.25,  0.23] | [-0.27,  0.32] | [-0.36,  0.37] | [-0.46, 0.45] #> correct_voicingvoiceless                        | [ 0.01,  0.87] | [-0.01,  0.93] | [-0.16,  1.02] | [-0.29,  1.16] | [-0.47, 1.31] #> repetitiontyperepeated                          | [ 0.07,  0.25] | [ 0.07,  0.26] | [ 0.03,  0.27] | [ 0.00,  0.30] | [-0.03, 0.34] #> correct_voicingvoiceless:repetitiontyperepeated | [-0.35, -0.11] | [-0.36, -0.09] | [-0.39, -0.05] | [-0.42,  0.00] | [-0.49, 0.03]  plot(hdi_range, show_intercept = T)"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/install-brms.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"How to install brms and dependecies","text":"fit model brms, package calls Rstan R interface statistical programming language Stan. nice thing brms uses syntax specifying model formulae based syntax commonly known lme4 package. lme4-like syntax brms converted Stan code automatically, won’t learn Stan.1 Stan built programming language C++ models compiled using C++ run. taken care brms, just need run brm(...) brms magic. However, since models compiled C++, need set computer can use C++. done , installing brms, procedure depends operating system. Continue reading detailed instructions.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/install-brms.html","id":"getting-help","dir":"Articles","previous_headings":"","what":"Getting help","title":"How to install brms and dependecies","text":"get stuck point installation process, please contact us learnb4ss@gmail.com soon possible. -importance start workshop everything works fine computer, strongly recommend go installation process early possible (e.g. least two weeks prior workshop).","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/install-brms.html","id":"r-and-rstudio","dir":"Articles","previous_headings":"","what":"R and RStudio","title":"How to install brms and dependecies","text":"⚠️ IMPORTANT! strongly recommend update R 4.x RStudio latest release. combo ensure smoother installation process use. able provide support R versions earlier 4.0. install/update R, find installation disk depending operating system : https://cloud.r-project.org. (macOS M1 chip, make sure install R 4.1.0 arm64) also highly recommend installing RStudio, provide interactive RMarkdown files additional resources best viewed used RStudio. Please, download RStudio update latest version: https://www.rstudio.com/products/rstudio/download/#download. following sections explain configure install software necessary workshop activities. instructions vary depending operating system, check section relevant . perform following steps: Configure C++ toolchain (needed Stan). Install Rstan (Stan). Install brms. already installed brms dependencies , can skip installation just check everything works running example(stan_model, package = \"rstan\", run.dontrun = TRUE) console.","code":""},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/articles/install-brms.html","id":"configure-the-c-toolchain","dir":"Articles","previous_headings":"macOS","what":"Configure the C++ toolchain","title":"How to install brms and dependecies","text":"configure C++ toolchain macOS, just need install Xcode Command Line Tools gfortran. First, files ~/.R/Makevars /~/.Renviron, save copy different location (like desktop) delete . don’t know files , go Finder > Go > Home, takes home directory, press SHIFT + CMD + . show hidden files (.e. files starting full stop .) can’t see already. Now search among hidden files see folder named .R/ /file .Renviron, copy desktop delete original copies home directory. important configurations want keep, can try put files back completed installation brms checked everything works fine. install Xcode Command Line Tools, run following Terminal (open Finder > Applications > Terminal, type following command press ENTER): macOS download install Xcode CLT (take , make sure stable internet connection). Mac Intel chip, need install gfortran v8.2 (Mojave) independent macOS version. can download installer https://github.com/fxcoudert/gfortran--macOS/releases/tag/8.2. Mac Apple M1 chip, need install gfortran v11. can download : https://github.com/fxcoudert/gfortran--macOS/releases/tag/11-arm-alpha2 (download .pkg package listed Assets bottom page).","code":"xcode-select --install"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/install-brms.html","id":"install-rstan","dir":"Articles","previous_headings":"macOS","what":"Install Rstan","title":"How to install brms and dependecies","text":"Just case previously tried install Rstan without success, run following code clean installations configuration files. Restart R. Assuming using R 4.0 later, just run R console: Now, verify installation running: model compiles starts sampling like gif , set.  case get following warning, safe ignore .","code":"remove.packages(\"rstan\") if (file.exists(\".RData\")) file.remove(\".RData\") install.packages(\"rstan\") example(stan_model, package = \"rstan\", run.dontrun = TRUE) # Warning message: # In system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = TRUE) : #  'C:/rtools40/usr/mingw_/bin/g++' not found"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/install-brms.html","id":"install-brms","dir":"Articles","previous_headings":"macOS","what":"Install brms","title":"How to install brms and dependecies","text":"Install brms CRAN simply : 🎉 Congrats! now ready run Bayesian regressions.","code":"install.packages(\"brms\")"},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/articles/install-brms.html","id":"configure-the-c-toolchain-1","dir":"Articles","previous_headings":"Windows","what":"Configure the C++ toolchain","title":"How to install brms and dependecies","text":"configure C++ toolchain, follow instructions R version (3.6 4.0): https://github.com/stan-dev/rstan/wiki/Configuring-C---Toolchain--Windows.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/install-brms.html","id":"install-rstan-1","dir":"Articles","previous_headings":"Windows","what":"Install Rstan","title":"How to install brms and dependecies","text":"Just case previously tried install Rstan without success, run following code clean installations configuration files. Restart R. just run: Now, verify installation running: model compiles starts sampling like gif , set.  case get following warning, safe ignore .","code":"remove.packages(\"rstan\") if (file.exists(\".RData\")) file.remove(\".RData\") install.packages(\"rstan\", repos = \"https://cloud.r-project.org/\", dependencies = TRUE) example(stan_model, package = \"rstan\", run.dontrun = TRUE) # Warning message: # In system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = TRUE) : #  'C:/rtools40/usr/mingw_/bin/g++' not found"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/install-brms.html","id":"install-brms-1","dir":"Articles","previous_headings":"Windows","what":"Install brms","title":"How to install brms and dependecies","text":"Install brms CRAN simply : 🎉 Congrats! now ready run Bayesian regressions.","code":"install.packages(\"brms\")"},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/articles/install-brms.html","id":"install-rstan-and-configure-the-c-toolchain","dir":"Articles","previous_headings":"Linux","what":"Install Rstan and configure the C++ toolchain","title":"How to install brms and dependecies","text":"Linux, option install pre-built Rstan binary build source. decide install pre-built binary, follow instructions : https://github.com/stan-dev/rstan/wiki/Configuring-C-Toolchain--Linux, Rstan. rather build source, : follow instructions (): https://github.com/stan-dev/rstan/wiki/Configuring-C-Toolchain--Linux- build source following instructions : https://github.com/stan-dev/rstan/wiki/Installing-RStan--Source#linux","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/install-brms.html","id":"install-brms-2","dir":"Articles","previous_headings":"Linux","what":"Install brms","title":"How to install brms and dependecies","text":"Install brms CRAN simply : 🎉 Congrats! now ready run Bayesian regressions.","code":"install.packages(\"brms\")"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/install-learnb4ss.html","id":"usage","dir":"Articles","previous_headings":"","what":"Usage","title":"How to install the learnB4SS package","text":"workshop, able follow along attaching package learnB4SS: Now can use function open_slides() open session slides browser open_exercise() open exercise files. use polite data throughout workshop. can load data :","code":"library(learnB4SS) # Open slides of session 00 open_slides(00)  # Open exercise of session 03 open_exercise(03) data(\"polite\")  # Learn about the data ?polite"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/understand-priors.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Understand and explore priors","text":"vignette digs bit deeper priors different types outcome/response variables. cover outcome variables follow distributions: normal/Gaussian, log-normal, binomial/Bernoulli, poisson, beta.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/understand-priors.html","id":"normalgaussian","dir":"Articles","previous_headings":"","what":"Normal/Gaussian","title":"Understand and explore priors","text":"Let’s start outcome variable distributed according normal/Gaussian distribution. fact, hardly ever work truly normally distributed variables. example look data set lexical emotional valence. emotion tibble contains list 1000 English words (word) emotional valence (valence). Emotional valence given number -4 +4, correspond “bad valence” “good valence” respectively. variable valence principle distributed according normal/Gaussian distribution (simply “normal” now ). NOTE probability distribution outcome variable (aka likelihood, family) chosen based visual inspection, based conceptual principles. Another word warning plotting continuous outcome variables geom_density() allow us assess distribution variable follows, data might generated mixture distributions. quick plot outcome variable just used ensure data ok (.e. doesn’t contain errors).  seems good. Now, want simply model distribution valence. valence follows normal distribution: \\[valence_i \\sim Normal(\\mu, \\sigma)\\] fact want estimate \\(\\mu\\) \\(\\sigma\\) data. need set prior probability distribution (simply prior) \\(\\mu\\) one prior \\(\\sigma\\). go-prior distribution \\(\\mu\\) yet another normal distribution, \\(\\mu_1\\) \\(\\sigma_1\\). \\[\\mu \\sim Normal(\\mu_1, \\sigma_1)\\] general rule, recommend using -called regularising priors setting \\(\\mu_1\\) 0. \\[\\mu \\sim Normal(0, \\sigma_1)\\] decide value assign \\(\\sigma_1\\), can use empirical rule: 95% CrI normal distribution interval contained within range defined \\(\\mu \\pm 2\\sigma\\). Since valence ranges -4 +4 definition, conservative approach \\(\\mu\\) allow values -8 +8. Since \\(\\mu_1 = 0\\), \\(\\sigma_1\\) \\(8/2 = 4\\). \\[\\mu \\sim Normal(0, 4)\\] \\(Normal(0, 4)\\) looks like.  now know prior set \\(\\mu\\). \\(\\sigma\\), one can choose normal distribution, Student-t distribution, Half-Cauchy distribution. go latter. \\[\\sigma \\sim HalfCauchy(x, \\gamma)\\] Half-Cauchy distributions can safely set \\(x = 0\\). \\[\\sigma \\sim HalfCauchy(0, \\gamma)\\] use empirical rule decide value \\(\\gamma\\) , rule works normal distributions. Instead, can use inverseCDF() function HDInterval package (see function documentation full explanation). don’t really idea standard deviation valence scores might , calculate standard deviation data (cheating!). just go quite weakly informative prior. functions returns lower upper boundary 95% HDI (Highest Density Interval). range [0, 25] standard deviation encompasses large range values, given range valence ([-4, +4]), making prior weakly informative one. prior \\(\\sigma\\) : \\[\\sigma \\sim HalfCauchy(0, 1)\\] sum : \\[ \\begin{aligned} valence_i & \\sim Normal(\\mu, \\sigma)\\\\ \\mu & \\sim Normal(0, 4)\\\\ \\sigma & \\sim HalfCauchy(0, 1) \\end{aligned} \\] corresponding code \\(valence_i \\sim Normal(\\mu, \\sigma)\\) : can check priors specify: brms code setting priors : point normally prior predictive checks. show next example. made sure prior predictive checks fine, go run model priors (run ).","code":"emotion #> # A tibble: 1,000 × 2 #>    word         valence #>    <chr>          <dbl> #>  1 classroom      0.57  #>  2 worker         0.95  #>  3 climb          0.62  #>  4 photographer   1.71  #>  5 downward      -1     #>  6 loose         -0.480 #>  7 scanner        0.57  #>  8 kooky          1.42  #>  9 credence       0.79  #> 10 fog            0.770 #> # ℹ 990 more rows emotion %>%   ggplot(aes(valence)) +   geom_density() +   geom_rug(alpha = 0.5) x <- seq(-15, 15, by = 0.01) y <- dnorm(x, 0, 4)  ggplot() +   aes(x, y) +   geom_line() # phcauchy is from the extraDistr package # If you are wondering why 0.025 and 0.975, that is the range that gives you # a 95% CrI/HDI. # Note that the phcauchy function calls gamma \"sigma\" but it means the same here. inverseCDF(c(0.025, 0.975), phcauchy, sigma = 1) #> [1]  0.03929027 25.45168269 m1 <- brm(   valence ~ 1,   data = emotion,   family = gaussian() ) get_prior(   valence ~ 1,   data = emotion,   family = gaussian() ) #>                   prior     class coef group resp dpar nlpar lb ub  source #>  student_t(3, 0.7, 2.5) Intercept                                  default #>    student_t(3, 0, 2.5)     sigma                             0    default priors_1 <- c(   # class Intercept = mu   prior(norma(0, 4), class = Intercept),   # no need to specify that cauchy should be half   # that is done automatically by brms   prior(cauchy(0, 1), class = sigma) ) m1 <- brm(   valence ~ 1,   data = emotion,   family = gaussian(),   prior = priors_1 )"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/understand-priors.html","id":"log-normal","dir":"Articles","previous_headings":"","what":"Log-normal","title":"Understand and explore priors","text":"Several measures can take positive values, like segment duration, tend skewed probability distribution. One type distribution log-normal distribution. illustrate run Bayesian regression model log-normal data, use data set vowel duration. Segment durations known follow log-normal distribution, since durations can positive. characteristics produces typical right skew log-normal data. Let’s look data. let’s plot raw vowel duration.  can start simple formula: \\[vow\\_dur_i \\sim Lognormal(\\mu_i, \\sigma)\\] Vowel duration distributed according log-normal distribution, mean \\(\\mu\\) standard deviation \\(\\sigma\\). want estimate \\(\\mu\\) \\(\\sigma\\) data. time though, also predictors interested : C2 voicing (voiceless, voiced), C2 place articulation (coronal, velar), vowel (//, /o/, /u/), speech rate (syllables per second). predictors linear model assumed effect \\(\\mu\\). following formula \\(\\mu\\) make evident: \\[ \\begin{aligned} \\mu_i & = \\alpha + \\beta_1 \\times c2voicing_i + \\beta_2 \\times c2poa_i \\\\ & + \\beta_3 \\times vowelO_i + \\beta_4 \\times vowelU_i \\\\ & + \\beta_5 \\times srate_i \\end{aligned} \\] \\(\\alpha\\) parameter model intercept, \\(\\beta\\) parameters coefficients predictors. parameters need set prior. continued…","code":"glimpse(vowels) #> Rows: 886 #> Columns: 9 #> $ item          <dbl> 20, 2, 11, 1, 15, 10, 13, 3, 14, 19, 4, 6, 16, 17, 5, 23… #> $ speaker       <chr> \"it01\", \"it01\", \"it01\", \"it01\", \"it01\", \"it01\", \"it01\", … #> $ word          <chr> \"pugu\", \"pada\", \"poco\", \"pata\", \"boco\", \"podo\", \"boto\", … #> $ v1_duration   <dbl> 95.23720, 138.96844, 126.93226, 127.49888, 132.33310, 12… #> $ c2_voicing    <chr> \"voiced\", \"voiced\", \"voiceless\", \"voiceless\", \"voiceless… #> $ vowel         <chr> \"u\", \"a\", \"o\", \"a\", \"o\", \"o\", \"o\", \"a\", \"o\", \"u\", \"a\", \"… #> $ c2_place      <chr> \"velar\", \"coronal\", \"velar\", \"coronal\", \"velar\", \"corona… #> $ speech_rate   <dbl> 4.893206, 5.015636, 4.819541, 5.031662, 5.063435, 5.0632… #> $ speech_rate_c <dbl> -0.55937531, -0.43694485, -0.63303978, -0.42091937, -0.3… vowels %>%   ggplot(aes(v1_duration)) +   geom_density() +   geom_rug()"},{"path":"https://learnb4ss.github.io/learnB4SS/articles/why-bayes.html","id":"practical-reasons","dir":"Articles","previous_headings":"","what":"Practical reasons","title":"Why Bayesian?","text":"Fitting frequentist models can lead anti-conservative p-values (.e. increased false positive rates, Type-error rates: effect yet got significant p-value). interesting example non-technically inclined reader can found : https://365datascience.com/bayesian-vs-frequentist-approach/. LMER tends sensitive small sample sizes Bayesian models (small sample sizes, Bayesian models return estimates greater uncertainty, conservative approach). simple models return similar estimates frequentist Bayesian statistics, cases complex models won’t fit run frequentist packages like lme4, especially without adequate enough sample sizes. BRMs always converge, LMERs don’t always . LMERs require much work BRMs, although common practice skip necessary steps fitting LMERs, gives impression fitting LMERs quicker process. Factoring time needed run MCMCs BRMs, LMER still perform robust perspective power analyses post-hoc model checks. BRMs, can reuse posterior distributions previous work include knowledge priors Bayesian analysis. feature effectively speeds discovery process (getting real value estimate interest faster). can embed previous knowledge BRMs can’t LMERs.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/articles/why-bayes.html","id":"conceptual-reasons","dir":"Articles","previous_headings":"","what":"Conceptual reasons","title":"Why Bayesian?","text":"LMERs provide evidence difference groups, evidence reject null hypothesis. frequentist CI can tell us , run study multiple times, n percent time CI include real value (don’t know whether CI got study one 100-n percent CIs CONTAIN real value). hand, Bayesian CrI ALWAYS tells us real value within certain range n percent probability. (course conditional model data, true frequentist Bayesian models alike). , LMER really just gives point estimate, BRMs give range values. BRM can compare hypothesis, just null vs alternative. (Although can use information criteria LMER). LMER based imaginary set experiments never actually carry . BRM converge towards true value long run. LMER . course, merits fitting LMERs, example corporate decisions, ’ll still lot work. main conceptual difference LMER BRMs answer different questions (basic) scientists generally interested questions BRMs can answer LMER .","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Stefano Coretta. Author, maintainer. Joseph V. Casillas. Author. Timo Roettger. Author.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Coretta S, Casillas J, Roettger T (2023). learnB4SS: Learning materials learnB4SS workshop. https://github.com/learnB4SS/learnB4SS, https://learnb4ss.github.io/learnB4SS/.","code":"@Manual{,   title = {learnB4SS: Learning materials for the learnB4SS workshop},   author = {Stefano Coretta and Joseph V. Casillas and Timo Roettger},   year = {2023},   note = {https://github.com/learnB4SS/learnB4SS, https://learnb4ss.github.io/learnB4SS/}, }"},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/index.html","id":"learning-materials","dir":"","previous_headings":"","what":"Learning materials","title":"Learning materials for the learnB4SS workshop","text":"can find learning materials workshop Learn Bayesian Analysis Speech Sciences (learnB4SS), form R package. dates info, see workshop homepage.","code":""},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/index.html","id":"prerequisites","dir":"","previous_headings":"Let’s get started!","what":"Prerequisites","title":"Learning materials for the learnB4SS workshop","text":"first step install brms dependencies. haven’t yet done , check installation instructions .","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/index.html","id":"workshop-materials","dir":"","previous_headings":"Let’s get started!","what":"Workshop materials","title":"Learning materials for the learnB4SS workshop","text":"save hassle, created Starter Kit, can download : Go download page. download page also contains instructions get set . NOTE. kit just convenient way setting RStudio project can use workshop. comfortable RStudio, can choose set RStudio project instead downloading kit. choose use Starter Kit, can just follow installation instructions learnB4SS package. Note package materials best used RStudio.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/index.html","id":"check-list","dir":"","previous_headings":"","what":"Check list","title":"Learning materials for the learnB4SS workshop","text":"summary prepping steps. Install brms dependencies (essential). Download Starter Kit (optional) install learnB4SS (essential). Hang us Slack (link instructions provided via email). Prepare snacks refreshments keep energy workshop (unfortunately can’t provide ).","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/emotion.html","id":null,"dir":"Reference","previous_headings":"","what":"Word emotional valence — emotion","title":"Word emotional valence — emotion","text":"tibble contains data Warriner 2013.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/emotion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Word emotional valence — emotion","text":"","code":"emotion"},{"path":"https://learnb4ss.github.io/learnB4SS/reference/emotion.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Word emotional valence — emotion","text":"tibble 1000 rows 2 variables: word Word. valency_z Emotional valency word (z-scores).","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/emotion.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Word emotional valence — emotion","text":"Warriner, . B., Kuperman, V., & Brysbaert, M. 2013. Norms     valence, arousal, dominance 13,915 English lemmas. Behavior     research methods, 45(4), 1191–1207.     https://doi.org/10.3758/s13428-012-0314-x","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/incomplete.html","id":null,"dir":"Reference","previous_headings":"","what":"Incomplete neutralisation in German — incomplete","title":"Incomplete neutralisation in German — incomplete","text":"tibble contains data Roettger et al. 2014.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/incomplete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Incomplete neutralisation in German — incomplete","text":"","code":"incomplete"},{"path":"https://learnb4ss.github.io/learnB4SS/reference/incomplete.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Incomplete neutralisation in German — incomplete","text":"tibble 6144 rows 8 variables: order Trial number. speaker_voice Speaker voice ID. item_pair ID lexical pair (voiceless/voiced). RT Reaction time. correct Whether listener correctly categorised word. correct_voicing underlying voicing category word. listener listener ID. repetitiontype Whether word heard first time subsequent times.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/incomplete.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Incomplete neutralisation in German — incomplete","text":"T.B. Roettger, B. Winter, S. Grawunder, J. Kirby, M. Grice. 2014.     Assessing incomplete neutralization final devoicing German.     Journal Phonetics 43. 11-25     https://doi.org/10.1016/j.wocn.2014.01.002.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/learnB4SS-package.html","id":null,"dir":"Reference","previous_headings":"","what":"learnB4SS: Learning materials for the learnB4SS workshop — learnB4SS-package","title":"learnB4SS: Learning materials for the learnB4SS workshop — learnB4SS-package","text":"provides learning materials workshop 'Learn Bayesian Analysis Speech Sciences'.","code":""},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/reference/learnB4SS-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"learnB4SS: Learning materials for the learnB4SS workshop — learnB4SS-package","text":"Maintainer: Stefano Coretta stefano.coretta@gmail.com (ORCID) Authors: Joseph V. Casillas (ORCID) Timo Roettger (ORCID)","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/open_exercise.html","id":null,"dir":"Reference","previous_headings":"","what":"Open exercise file. — open_exercise","title":"Open exercise file. — open_exercise","text":"Call function session number argument exercise file copied working directory opened editor.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/open_exercise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open exercise file. — open_exercise","text":"","code":"open_exercise(session)"},{"path":"https://learnb4ss.github.io/learnB4SS/reference/open_exercise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open exercise file. — open_exercise","text":"session Session number number.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/open_exercise.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Open exercise file. — open_exercise","text":"Nothing. Used side effects.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/open_exercise.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Open exercise file. — open_exercise","text":"following sessions exercises: - Session 03. - Session 06. - Session 08. can also open fully worked-analysis `open_exercise(\"full\")`","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/open_exercise.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Open exercise file. — open_exercise","text":"","code":"if (FALSE) { open_exercise(3)  open_exercise(\"full\") } # The exercise of Session 03 will open for editing."},{"path":"https://learnb4ss.github.io/learnB4SS/reference/open_slides.html","id":null,"dir":"Reference","previous_headings":"","what":"Open slides in browser. — open_slides","title":"Open slides in browser. — open_slides","text":"Call function session number argument slides session open browser (internet connection required).","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/open_slides.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open slides in browser. — open_slides","text":"","code":"open_slides(session)"},{"path":"https://learnb4ss.github.io/learnB4SS/reference/open_slides.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open slides in browser. — open_slides","text":"session Session number number.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/open_slides.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Open slides in browser. — open_slides","text":"Nothing. Used side effects.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/open_slides.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Open slides in browser. — open_slides","text":"","code":"if (FALSE) { open_slides(0) }"},{"path":"https://learnb4ss.github.io/learnB4SS/reference/polite.html","id":null,"dir":"Reference","previous_headings":"","what":"The phonetic profile of Korean formal and informal speech registers — polite","title":"The phonetic profile of Korean formal and informal speech registers — polite","text":"tibble contains data study discussed Winter Grawunder 2012.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/polite.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The phonetic profile of Korean formal and informal speech registers — polite","text":"","code":"polite"},{"path":"https://learnb4ss.github.io/learnB4SS/reference/polite.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"The phonetic profile of Korean formal and informal speech registers — polite","text":"tibble 224 rows 29 variables: subject Subject unique identifier [categorical]. gender Gender subject [categorical]. birthplace Birth place subject [categorical]. musicstudent subject music training? [binary: yes, ] scenario Unique identifier different items. task Task type [categorical: = mailbox task vs        dct = discourse completion task]. mailbox task people        left note somebody's mailbox, discourse completion        task prompted role-play start conversation. attitude Attitude [binary: polite vs informal]. total_duration Total duration utterances seconds [numeric]. articulation_rate Number syllables per second [numeric]. f0mn Mean fundamental frequency (f0) [numeric]. f0sd Stadard deviation fundamental frequency [numeric]. f0range Minimum maximum fundamental frequency [numeric]. inmn Mean intensity [numeric]. insd Standard deviation intensity [numeric]. inrange Minimum maximum fundamental frequency [numeric]. shimmer Local shimmer (likewise normalized amplitude difference consecutive periods) [numeric]. jitter Local jitter (bsolute period--period difference divided average period) [numeric]. HNRmn Mean Harmonics--Noise Ratio [numeric]. H1H2 Difference first second harmonic (H1-H2) [numeric]. breath_count Number audible breath intakes [count]. filler_count Number oral fillers like \"oh/ah\" [count]. hiss_count Number noisy breath intakes [count]. nasal_count Number nasal fillers like \"mh/nh\" [count]. sil_count Number silent pauses [count]. ya_count Number occurences interjection \"ya\" (informal) [count]. yey_count Number occurences interjection \"yey\" (polite) [count].","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/polite.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"The phonetic profile of Korean formal and informal speech registers — polite","text":"Bodo Winter, Sven Grawunder. 2012. phonetic profile Korean     formal informal speech registers, Journal Phonetics 40(6). 808-815.     https://doi.org/10.1016/j.wocn.2012.08.006","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/vowels.html","id":null,"dir":"Reference","previous_headings":"","what":"Vowel duration and consonant voicing in Italian — vowels","title":"Vowel duration and consonant voicing in Italian — vowels","text":"tibble contains data Coretta 2020.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/vowels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vowel duration and consonant voicing in Italian — vowels","text":"","code":"vowels"},{"path":"https://learnb4ss.github.io/learnB4SS/reference/vowels.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Vowel duration and consonant voicing in Italian — vowels","text":"tibble 6144 rows 8 variables: item Trial number. speaker Speaker ID. word Target word. v1_duration Duration V1 milliseconds. c2_voicing Voicing C2 (voiceless, voiced). vowel Target vowel. c2_place Place articulation C2. speech_rate Number syllables per second. speech_rate_c Centred speech rate.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/reference/vowels.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Vowel duration and consonant voicing in Italian — vowels","text":"Coretta, Stefano. 2020. Vowel duration consonant voicing: production study.     PhD Thesis. University Manchester. https://doi.org/10.17605/OSF.IO/W92ME","code":""},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/news/index.html","id":"added-1-0-7-9000","dir":"Changelog","previous_headings":"","what":"Added","title":"learnB4SS v1.0.7.9000","text":"contrasts.Rmd vignette dummy coding factor contrasts.","code":""},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/news/index.html","id":"added-1-0-7","dir":"Changelog","previous_headings":"","what":"Added","title":"learnB4SS v1.0.7","text":"emotion tibble lexical emotional valence data Warriner 2013. incomplete tibble perceptual data incomplete voicing neutralisation Roettger et al. 2014. full-analyses.Rmd vignette.","code":""},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/news/index.html","id":"fixed-1-0-6","dir":"Changelog","previous_headings":"","what":"Fixed","title":"learnB4SS v1.0.6","text":"Installation package requires dependencies = TRUE.","code":""},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/news/index.html","id":"changed-1-0-6","dir":"Changelog","previous_headings":"","what":"Changed","title":"learnB4SS v1.0.6","text":"Dependencies DESCRIPTION.","code":""},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/news/index.html","id":"added-1-0-4","dir":"Changelog","previous_headings":"","what":"Added","title":"learnB4SS v1.0.4","text":"Vignette reasons switching Bayesian stats.","code":""},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/news/index.html","id":"added-1-0-3","dir":"Changelog","previous_headings":"","what":"Added","title":"learnB4SS v1.0.3","text":"Info recordings live sessions.","code":""},{"path":"https://learnb4ss.github.io/learnB4SS/news/index.html","id":"fixed-1-0-3","dir":"Changelog","previous_headings":"","what":"Fixed","title":"learnB4SS v1.0.3","text":"Add yet another missing dependecy.","code":""},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/news/index.html","id":"fixed-1-0-2","dir":"Changelog","previous_headings":"","what":"Fixed","title":"learnB4SS v1.0.2","text":"Add missing dependency.","code":""},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/news/index.html","id":"added-1-0-1","dir":"Changelog","previous_headings":"","what":"Added","title":"learnB4SS v1.0.1","text":"✨ - Half cauchy example ex_03.","code":""},{"path":[]},{"path":"https://learnb4ss.github.io/learnB4SS/news/index.html","id":"added-1-0-0","dir":"Changelog","previous_headings":"","what":"Added","title":"learnB4SS v1.0.0","text":"🎉 - First package release!","code":""}]
